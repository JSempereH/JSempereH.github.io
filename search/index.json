[{"content":"Hi!\nI wanted to share my MSc thesis Privacy-Enhancing Machine Learning (2024), based mostly on my first year working at GMV.\nMy tutors were Carlos Gregorio Rodr√≠guez and Juan Miguel Au√±√≥n Garc√≠a (if you have the opportunity, I recommend working with them, I learnt a lot).\nHere is the PDF :)\nAnd here\u0026rsquo;s the song recommendation üòâ\n","date":"2025-11-02T00:00:00Z","image":"http://localhost:1313/posts/tfm/TFM_image_hu_1e92dfb8952a1f5c.png","permalink":"http://localhost:1313/posts/tfm/","title":"Privacy-Enhancing Machine Learning"},{"content":"Hi!\nI wanted to share my BSc thesis from 2023. I don‚Äôt think anyone will find much value in it, I just wanted to mix design and math and play around with it üôÇ\nI\u0026rsquo;ll keep it here, in my own little corner of the internet, as a memory of a time I hold close to my heart. A lot happened that year. I started living with my boyfriend in a tiny apartment in Madrid, studying A LOT, and working part-time to cover expenses. It was exhausting, but I miss it somehow.\nMy tutor was Marina Logares, an amazing mathematician I admire deeply. Not only is she brilliant at what she does, but she‚Äôs also an activist.\nWhen I first approached her, I told her I just wanted to explore whatever I felt like and make some art out of it. I even changed the topic halfway through because I got bored and wanted to try something else! Still, the committee actually valued it a lot. I got the highest grade! üòÇ\nBy the way, it\u0026rsquo;s in spanish. I\u0026rsquo;m too lazy to translate it. Thankfully, we have LLMs for that.\nThe PDF is here.\nAnd here\u0026rsquo;s the song recommendation üíõ\n","date":"2025-11-01T00:00:00Z","image":"http://localhost:1313/posts/tfg/Le_Chien_hu_b972ded2fde54e74.png","permalink":"http://localhost:1313/posts/tfg/","title":"Aproximaci√≥n de puntos v√≠a curvas de B√©zier"},{"content":"Our objective is to compact the representation of data while minimizing the information loss using linear transformations.\nPrincipal Component Analysis (PCA) Given $n$ data vector ${x_1, \\dots, x_n} \\in \\mathbb{R}^m$, assuming that are mean centered, and given a target number of dimensions $k \u0026lt; m$. PCA finds an orthonormal basis ${u_1, \\dots, u_k} \\in \\mathbb{R}^m$ that explains most of the variation of the projected data. Let $U \\in \\mathbb{R}^{m \\times k}$ the matrix whose columns are $u_j \\in \\mathbb{R}^m$ and $X \\in \\mathbb{R}^{m \\times n}$ is a matrix whose columns are the data vector $x_j \\in \\mathbb{R}^m$.\n$U$ encodes the original data $X$ into a lower dimension $z = U^TX$, $z_j= u^T_j x_j$. How do we choose $U$? There are two main approaches:\nReconstruction error: The reconstruction data is $U (U^T X) = \\tilde{X}$, therefore we want to minimize $||X - \\tilde{X}||$ for some norm (we\u0026rsquo;ll see that later). Therefore, we want to minimize the reconstruction error $$ \\min_{U \\in \\mathbb{R}^{m \\times k}} \\sum_{i=1}^n ||x_i - UU^T x_i||^2 $$\nProjected variance: Since the data is mean centered, $\\hat{\\mathbb{E}}[X] = 0$, we want to maximize the variance of the projected data $$ \\max_{U \\in \\mathbb{R}^{m \\times k}, \\hspace{.5 em}U^T U = I} \\hat{\\mathbb{E}} [\\lVert U^T X \\rVert^2] $$\nActually, these two approaches are equivalent. By the Pythagorean decomposition, given a vector $x \\in \\mathbb{R}^m$,\n$$ x = UU^Tx + (I - UU^T)x $$\nFor simplicity, let\u0026rsquo;s consider $k=1$, following the second approach we want to find the direction where the variance of the projected data is maximized. The columns vectors of $U$ must be orthonormal $(U^T U = I_k)$ to avoid degenerate solutions $(|u|=\\infty)$. Take into account that since $U$ is not a square matrix, this doesn\u0026rsquo;t imply that $UU^T = I_m$, $UU^T$ will be a projection matrix into a $k$-dimensional subspace.\n$$ \\begin{aligned} \u0026amp;\\max_{\\lVert \\mathbf{u}\\rVert=1} \\hat{\\mathbb{E}} \\Big[ \\big( \\mathbf{u}^\\top \\mathbf{x} \\big)^2 \\Big] \\\\ \u0026amp;= \\max_{|\\mathbf{u}|=1} \\frac{1}{n} \\sum_{i=1}^n \\big( \\mathbf{u}^\\top \\mathbf{x}_i \\big)^2 \\\\ \u0026amp;= \\max_{\\lVert \\mathbf{u} \\rVert =1} \\frac{1}{n} \\lVert \\mathbf{u}^\\top \\mathbf{X} \\rVert^2 \\\\ \u0026amp;= \\max_{|\\mathbf{u}|=1} \\mathbf{u}^\\top \\Big( \\frac{1}{n} \\mathbf{X} \\mathbf{X}^\\top \\Big) \\mathbf{u} \\\\ \u0026amp;= \\text{largest eigenvalue of } S \\overset{\\text{def}}{=} \\frac{1}{n} \\mathbf{X} \\mathbf{X}^\\top \\\\ \u0026amp;\\text{where } S \\text{ is the sample covariance matrix of the data.} \\end{aligned} $$\nBecause the variance of the projected data (in the direction of $u$) is given by:\n$$ \\frac{1}{n} \\sum_{i=1}^n (u^T x_i - u^T \\bar{x})^2 = u^T S u $$\nSince $u^T u = | u|_2^2 = 1$, the optimization problem is equivalent to\n$$ \\begin{aligned} \u0026amp; \\max_u \u0026amp;\u0026amp; u^T S u \\\\ \u0026amp; \\text{subject to} \u0026amp;\u0026amp; \\lVert u\\rVert_2^2 = 1, \\quad u \\in \\mathbb{R}^k \\end{aligned} $$\nThe Lagrangian of this problem is $\\mathcal{L}(u) = u^T S u - \\lambda(u^T u - 1)$, where $\\lambda$ is the Lagrange multiplier. The gradient of $\\mathcal{L}(u)$ with respect to $u$ is:\n$$ \\frac{\\partial \\mathcal{L} (u)}{\\partial u} = S u - \\lambda u $$\nTherefore, $\\frac{\\partial \\mathcal{L} (u)}{\\partial u} = 0 \\leftrightarrow Su = \\lambda u$. Since $u \\neq 0$, $\\lambda$ is a eigenvalue. In the optimization problem, we have now\n$$ \\begin{aligned} \u0026amp; \\max_u \u0026amp;\u0026amp; \\lambda u^T u \\\\ \u0026amp; \\text{subject to} \u0026amp;\u0026amp; |u|_2^2 = 1, \\quad Su = \\lambda u \\quad u \\in \\mathbb{R}^k \\ \\end{aligned} $$\nSince $u^T u = |u|_2^2 = 1$, we just want to find the largest eigenvalue of the covariance matrix $S$.\nBy induction, it can be easily proved that we want to project the data in $k$-dimensional space, we need to find the $k$ eigenvectors of $S$ associated with the $k$ largest eigenvalues. This would be done supposing it holds for some general value $k$, and showing it consequently holds for $k+1$, imposing the orthonormality of the eigenvectors in the Lagrange formulation (see Section 16.1.2 from Bishop\u0026rsquo;s Deep Learning book, I really recommend this book ‚ò∫Ô∏è).\nSince $S$ is a symmetric positive semidefinite matrix, it has some nice propertities: we know there is an eigenvalue decomposition of the form $S = Q \\Lambda Q^T$, where $Q$ is an orthogonal matrix ($QQ^T = I$) and $\\Lambda$ is a diagonal matrix with nonnegative eintries $\\lambda_i$ such that $\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_k \\geq 0$. The objective function is then $u^T S u = u^T Q \\Lambda Q^T u = w^t \\Lambda w = \\sum_{i=1}^k \\lambda_i w_i^2$, which is constrained to $\\sum_{i=1}^k w_i^2 = 1$ since $|w|_2 = |Q^Tu|_2 = |u|_2 = 1$.\nSince $\\lambda_1 \\geq \\lambda_i$, $\\forall i \\neq 1$, we have that $w^* = e_1 = (1,0,\\dots,0)\\in \\mathbb{R}^k$. Therefore, $u^* = Q e_1 = q_1$, the first column of $Q$ (the eigenvector corresponding to the largest eigenvalue of $S$). The rest of the principal component vectors can be found by solving a sequence of similar optimization problems with the additional constraint that the solution must be orthogonal to all the previous solutions in the sequence. The solution of the $i-th$ problem is, inductively, $q_i$ (the $i-th$ eigenvector of $S$). In practice, $SVD$ already gives us the orthonormal basis we are looking for.\nNote that $u \\perp (x_i - uu^Tx_i) = 0$ by the Pythagorean decomposition\nTaking expectations\n$$ \\hat{\\mathbb{E}}[|x|^2] = \\hat{\\mathbb{E}}[|U^Tx|^2] + \\hat{\\mathbb{E}}[|x-UU^Tx|^2] $$\nWe can see that minimizing the reconstruction error is equivalent to maximizing the captured variance.\n$$ \\sum_{i=1}^n | x_i - uu^T x_i |^2 = \\sum_{i=1}^n (|x_i|^2 - (u^T x_i)^2) = constant - \\sum_{i=1}^n (u^Tx_i)^2 = constant - |u^T X|^2 $$\nSingular Value Decomposition (SVD) Let $X$ be the real $m \\times n$ matrix of rank $r$, $I_r$ the $r \\times r$ identity matrix. A singular value decomposition of $X$ is $X = U \\Sigma V^T$ where:\n$U \\in \\mathcal{M}_{m\\times r}(\\mathbb{R})$ such that $U^T U = I_r$. Note that the columns ${u_1,\\dots,u_r}$ form an orthonormal set. $V \\in \\mathcal{M}_{n \\times r}(\\mathbb{R})$ such that $V^T V = I_r$. Note that the columns ${v_1,\\dots, v_r}$ form an orthonormal set. $\\Sigma = \\begin{pmatrix} \\sigma_1 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; \\sigma_2 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; \\sigma_r \\\\ \\end{pmatrix}$ where $\\sigma_1 \\geq \\dots \\geq \\sigma_r \u0026gt; 0$ Every matrix $X \\in \\mathcal{M}_{m\\times n}(\\mathbb{R})$ has an SVD.\nSince $u_i \\in \\mathbb{R}^{m \\times 1}$ and $v_i^T \\in \\mathbb{R}^{1 \\times n}$, $X$ can be expressed as\n$$ X = \\sum_{i=1}^r \\sigma_i u_i v_i^T $$\nWe can extend $U$ to an $m \\times m$ orthogonal matrix $\\hat{U}$ by adding $m-r$ extra columns, we can expand $V$ to an $n \\times n$ orthogonal matrix $\\hat{V}$ adding $n-r$ extra columns, and we can expand $\\Sigma$ to an $m \\times n$ matrix $\\hat{\\Sigma}$ by adding extra entries that are all zero. We will refer to this factorization as the full SVD. Geometrically, we are applying a rotation to $X$, followed by a scaling, followed by another rotation. For now on, we will denote the full SDV just as $U\\Sigma V^T$.\nGiven any SVD of $X$, the singular values are the square roots of the nonzero eigenvalues of $X^TX$ or $XX^T$ (they have the same eigenvalues). From\n$$ X^T X V = \\left( U \\Sigma V^T \\right)^T \\left( U \\Sigma V^T \\right) V = V \\Sigma^T U^T U \\Sigma V^T V = V \\Sigma^T \\Sigma V^T V = V \\Sigma^2 $$\nit follows that each right singular vector $v_i$ of $X$ is an eigenvector of $X^TX$ with non-zero eigenvalue $\\sigma_i^2$ for $i=1,\\dots, r$. The remaining $n-r$ columns of $V$ span the eigenspace of $X^TX$ corresponding to the eigenvalue $0$. One can show a similar result for $X X^T$. For any full SDV $X = U \\Sigma V^T$, the columns of $U$ spans an orthonormal set of eigenvectors of $X X^T$ and the columns of $V$ spans an orthonormal set of eigenvectors of $X^T X$.\nLet\u0026rsquo;s introduce the Frobenius matrix norm:\n$$ \\begin{aligned} \u0026amp;\\lVert X \\rVert_F := \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n x_{ij}^2} = \\sqrt{trace(X^T X)} \\\\ \u0026amp;= \\sqrt{trace((U \\Sigma V^T)^T(U \\Sigma V^T))} = \\sqrt{trace(\\Sigma^T \\Sigma)} \\\\ \u0026amp;= \\sqrt{\\sigma_1^2 + \\dots \\sigma_r^2} \\end{aligned} $$\nGiven the SVD of $X = \\sum_{i=1}^r \\sigma_i u_i v_i^T$, $k \\leq r$, we can obtain a rank-$k$ matrix $X_k$ by truncating the SVD after the first $k$ terms:\n$$ X_k := \\sum_{i=1}^k \\sigma_i u_i v_i^T $$\nWe can see that $Img(X_k) = span(u_1,\\dots,u_k)$, $X_k$ has singular values $\\sigma_1, \\dots, \\sigma_k$ and\n$$ \\lVert X-X_k \\rVert_F = \\sqrt{\\sigma_{k+1}^2+ \\dots + \\sigma_r^2} $$\nBy Eckhart-Young theorem, $X_k$ is the best rank-$k$ approximation of $X$ w.r.t the Frobenius norm. Actually, this also applies to any unitarily invariant norm, like the spectral norm $||\\cdot||_2$. The explained variance ratio measures the relative error of this low-rank approximation:\n$$ \\frac{\\lVert X-X_k \\rVert_F^2}{\\lVert X \\rVert_F^2} = \\frac{\\sigma_{k+1}^2 + \\dots + \\sigma_r^2}{\\sigma_1^2 + \\dots + \\sigma_r^2} $$\nFor PCA we can use SVD: let\u0026rsquo;s assume we have the mean centered input data ${x_1, \\dots, x_n} \\in \\mathbb{R}^m$. If we set $k \\leq m$, we want to find $\\{ \\tilde{x}_1, \\dots, \\tilde{x}_n \\} \\in \\mathbb{R}^m$ with rank $\\leq k$ (they must lie in a subspace of dimension at most $k$), writing the mean-cetered data and the approximations as column matrices:\n$$ \\mathbf{X} := \\left( \\begin{array}{ccc} x_1 \u0026amp; \\cdots \u0026amp; x_n \\end{array} \\right),\\quad \\tilde{\\mathbf{X}} := \\left( \\begin{array}{ccc} \\tilde{x}_1 \u0026amp; \\cdots \u0026amp; \\tilde{x}_n \\end{array} \\right) $$\nThe reconstruction error is $\\left| X - \\tilde{X} \\right|_F^2$. By Eckhart-Young Theorem an optimal choice of $\\tilde{X}$ is the matrix $X_k$ via a truncated SVD.\nIf $U_k$ is the $n \\times k$ matrix whose columns are the top $k$ left singular vectors of $X$, then\n$$ X_k = U_k U_k^T X = X_k Z $$\nwhere $Z = U_k^T X$ are the coefficients that respectively approximate each mean centered data point as a linar combination of the top $k$ left eigenvectors.\nLinear Autoencoders and PCA equivalence An autoencoder is a type of neural network trained to reconstruct its input. It consists of two components:\nAn encoder function $f: \\mathbb{R}^m \\to \\mathbb{R}^k$ that maps the input to a latent (compressed) representation. A decoder function $g: \\mathbb{R}^k \\to \\mathbb{R}^m$ that reconstructs the input from the latent representation. For a linear autoencoder, both the encoder and decoder are linear maps:\n$$ f(x) = W_e^T x, \\quad g(z) = W_d^T z $$\nwhere $W_e \\in \\mathbb{R}^{m \\times k}$ and $W_d \\in \\mathbb{R}^{m \\times k}$. Given a dataset of $n$ mean-centered data vectors ${x_1, \\dots, x_n} \\in \\mathbb{R}^m$, we arrange them into a matrix:\n$$ X = \\begin{pmatrix} x_1 \u0026amp; \\cdots \u0026amp; x_n \\end{pmatrix} $$\nThe reconstruction is:\n$$ \\tilde{X} = W_d^T W_e^T X $$\nThe objective is to find $W_e$ and $W_d$ that minimize the reconstruction error, typically measured using the spectral norm:\n$$ \\mathcal{L}(W_e, W_d) = \\left| X - \\tilde{X} \\right|_2^2 = \\left| X - W_d^T W_e^T X \\right|_2^2 $$\nBut that objective function to minimize is similar to the one we saw in the reconstruction-error formulation for the PCA. However, here we are not constraining the matrices $W_e$ and $W_d$ as we did with $U$.\nConnection to PCA If the autoencoder has a single linear hidden layer, no biases, mean-centered input, and is trained with squared error loss, the optimal solution for the weight matrices spans the same subspace as the top $k$ principal components of $X$, (see Baldi and Hornik, 1989). Actually, even if the units are not linear, the networks projects the input onto the subspace spanned by the first $k$ principal components of the input (see Bourlard and Kamp, 1988).\nIn particular:\nThe rows of $W_e^T$ span the same subspace as the top $k$ left singular vectors of $X$ (i.e., $U_k$ from the truncated SVD of $X$). The reconstructed data matrix $\\tilde{X}$ is the projection of $X$ onto the subspace spanned by these top $k$ components. Formally, if $U_k$ contains the top $k$ left singular vectors of $X$, the optimal reconstruction is:\n$$ \\tilde{X} = U_k U_k^T X $$\nwhich is precisely the truncated SVD reconstruction (the PCA rank-$k$ approximation).\nThus, a linear autoencoder trained in this setting essentially learns the PCA solution: it projects the data onto a lower-dimensional linear subspace that captures the directions of highest variance, and then reconstructs the data from this projection.\nExample:\nThe following animation illustrates how a linear autoencoder progressively learns the principal direction of a 2D dataset.\nOn the left plot, the scatter plot shows the input 2D data points sampled from a normal distribution with a non-diagonal covariance matrix (i.e., correlated variables).\nThe green arrow represents the first principal component computed by PCA. This direction captures the largest variance in the data and remains fixed throughout the animation. The red arrow represents the first component (first row of the encoder‚Äôs weight matrix) learned by the autoencoder during training. This arrow evolves over epochs as the autoencoder updates its weights to better reconstruct the data. On the right plot, the training loss value (mean squared error) is plotted against the number of epochs. This shows how the reconstruction error decreases as the autoencoder progressively aligns its learned component with the true principal direction.\nBoth arrows are normalized to unit length in each frame to visualize only their direction (subspace) rather than their magnitude.\nThe learning rate is controlled by a scheduler that reduces it by half every 30 epochs, which helps stabilize convergence in the later stages of training.\nLinear Autoencoders don\u0026rsquo;t recover exact principal components PCA imposes:\nOrthonormality: $U_k^T U_k = I$ Ordering by explained variance (via eigenvalues) A linear autoencoder does not enforce:\nOrthogonality between encoding directions Ordering of the directions Unit norms for basis vectors Thus, even though it learns to project onto the same subspace, its basis vectors (rows of $W_e$ or columns of $W_e^T$) might differ from the true principal components.\nNote that even if we force $W_e = W_d^T$, the learned subspace basis vectors may still differ from exact PCA vectors. Since any rotation within the latent k-dimensional space preserves the reconstruction error, as Kunin, D. et al. (2019) states:\nWithout regularization, LAEs learn this subspace but cannot learn the principal directions themselves due to the symmetry of $\\mathcal{L}$ under the action group $GL_k(\\mathbb{R})$ of invertible $k \\times k$ matrices defined by $(W_d, W_e) \\mapsto (GW_d, W_e G^{-1})$: $$ X - (W_e G^{-1})(G W_d)X = X - W_e W_d X $$\nThey proved that LAEs with $L_2$-regularization do in fact learn the principal directions as the left singular vectors of the decoder, since the regularization reduces the symmetry group from $GL_k(\\mathbb{R})$ to the orthogonal group $O_k (\\mathbb{R})$, which preserves the structure of SVD. The loss is then defined as:\n$$ \\mathcal{L}_\\lambda(W_e, W_d) = \\mathcal{L}(W_e, W_d) + \\lambda (\\left| W_1 \\right|_F^2 + \\left| W_2 \\right|_F^2 ) $$\nAll stationary points satify $W_e = W_d^T$\nBao, X et al. 2020 proposed that the rows of $W_d$ and the columns of $W_e$ are penalized with differents weights (non-uniform $L_2$-regularization). Let $0 \u0026lt; \\lambda_1 \u0026lt; \\cdots \u0026lt; \\lambda_k$ and $\\Lambda = diag(\\lambda_1,\\dots, \\lambda_k)$. The proposed loss funcion is\n$$ \\mathcal{L}_{\\lambda\u0026rsquo;}(W_e, W_d) = \\left| X - W_d W_e X \\right|_F^2 + \\left| \\Lambda^{1/2} W_e \\right|_F^2 + \\left| W_d \\Lambda^{1/2} \\right|_F^2 $$\nThis loss function also satifies $W_e = W_d^T$ and the global minimum recovers the ordered, axis-aligned individual principal directions. However, the convergence is slow due to ill-conditioning at global optima using gradient-descent update rules. In the paper, a modification of the update rule is proposed to tackle this: the rotation augmented gradient (RAG).\nAs we have seen, the vectors of weights of a linear autoencoder form a basis set that spans the same subspace as PCA, however, these vectors are not orthogonal or normalized. Considering there are online PCA algorithms, there is no advantage in using a linear autoencoder. However, deep autoencoders with non-linear layers are not restricted to linear transformations, therefore it will better capture the latent space of complex, non-linear data thanks to its flexibility, at the cost of loosing the simple interpretation a PCA provides. You could also use Kernel PCA (or its variants), which can be more suitable if you are not working with Big Data, although it\u0026rsquo;s sensitive to the kernel you use.\nThat\u0026rsquo;s all for today :)\nIf you spot any error, please open an issue in the web\u0026rsquo;s repo. Thx!\n","date":"2025-10-03T00:00:00Z","permalink":"http://localhost:1313/posts/pca-lae/","title":"PCA is equivalent to a Linear Autoencoder"},{"content":"In this post I\u0026rsquo;d like to write down the steps to make the simplest Python project you could make. Mainly focused for research projects (e.g. people in academia). Therefore, there will be no pipelines (like Github Actions or Jenkins), nor any docs.\nVersion control Let\u0026rsquo;s say you have created your local directory: mkdir my-project. First, you need to init the version control. As of today, the most used tool is git.\n1 2 cd my-project git init You are probably using Github, Gitlab or Codeberg for hosting your repo. Let\u0026rsquo;s say you are using Github, and you have already created your repo there. Now you need to add the remote repository reference to your local Git:\n1 git remote add origin https://github.com/your-user/your-project.git This command is setting the name \u0026ldquo;origin\u0026rdquo; to that remote. You can get the URL of the remote Git repository directly on the web:\nAnother way of doing this is directly creating the Github repo and then clone it locally:\n1 git clone https://github.com/your-user/your-project.git In case it is a private repo, you will need to log in.\nYou will need to create a .gitignore file. This will tell Git which file it shouldn\u0026rsquo;t track. For example:\n1 2 3 4 5 6 7 8 9 10 11 12 # Ignore Python cache __pycache__/ *.pyc # Ignore virtual environment .venv/ # Ignore logs *.log # Ignore IDE files .vscode/ You can indicate whole directories (e.g. .venv/) or specific files, like all the files that end with .log: *.log.\nPackage manager I will use uv for this post, since I think it\u0026rsquo;s one of the best tools. You can also use pip, poetry, etc.\nIn case you don\u0026rsquo;t have it yet, you can install it with:\n1 curl -LsSf https://astral.sh/uv/install.sh | sh Let\u0026rsquo;s say you are already in your repo directory. Init uv with:\n1 uv init This will create the pyproject.toml, uv.lock, main.py (you will probably delete it), README.md, .python-version and a .gitignore (if you don\u0026rsquo;t have it yet).\npyproject.toml: this is a configuration file. It contains info about the project name, version, authors, dependencies, dev-dependencies (packages only needed for development), etc. For example: 1 2 3 4 5 6 7 8 [project] name = \u0026#34;my_project\u0026#34; version = \u0026#34;0.1.0\u0026#34; requires-python = \u0026#34;\u0026gt;=3.12\u0026#34; dependencies = [\u0026#34;numpy\u0026gt;=2.3.1\u0026#34;] [tool.uv] dev-dependencies = [\u0026#34;pytest\u0026#34;, \u0026#34;ruff\u0026#34;] uv.lock: this lockfile states exact versions of all dependencies, including transitive ones (those required by your direct dependencies). If you want to use a new dependency in your code (e.g. numpy) you can use:\n1 uv add numpy This will update pyproject.toml and uv.lock. If you want to install a dev-dependency (e.g. pytest), you use:\n1 uv add --dev pytest You can remove a package with uv remove. In case another collaborator is working in your project and they update the packages, you can sync your virtual environment with the dependencies listed in pyproject.toml and uv.lock with uv sync.\nIf you want to install a library locally (not changing the project\u0026rsquo;s dependencies), use uv pip. For example, if you want to create a notebook that uses your python module, but don\u0026rsquo;t want to add ipykernel to the list of dependencies, you may use:\n1 uv pip install ipykernel As a rule of thumb, try to minimize the number of dependencies in your project. Maintaining a project means updating the dependencies and handling the possible issues. The fewer the dependencies, the better.\nClean Python code Let\u0026rsquo;s say you will write your package my-project with its __init__.py (so it is treated as a package) and some code. Your directory will look something like this:\n1 2 3 4 5 6 7 8 9 . ‚îú‚îÄ‚îÄ my-project ‚îÇ ‚îú‚îÄ‚îÄ __init__.py ‚îÇ ‚îî‚îÄ‚îÄ my_code.py ‚îú‚îÄ‚îÄ pyproject.toml ‚îú‚îÄ‚îÄ README.md ‚îî‚îÄ‚îÄ uv.lock 2 directories, 5 files (Ignoring files starting with a .)\nBefore you start writing your code, it\u0026rsquo;s better to set up some Python tools that format and correct your code.\nLinter and formatter A linter will check your code for potential errors, bad practices, styles issues and bugs. And a formatter will reformat your code automatically, so you don\u0026rsquo;t have to worry about spaces or line breaks.\nRuff can be used for both linting and formatting, you may install it as a dev-package:\n1 uv add --dev ruff Pre-commit Instead of running ruff while you are developing the code, you could run it every time you try to commit some code using pre-commit. This way, it will format the the code and warn you if changes must be made. Install it also as a dev-package:\n1 uv add --dev pre-commit Now you need to create a .pre-commit-config.yaml file in the root of your project. The following config tell pre-commit to run ruff for linting and formatting:\n1 2 3 4 5 6 repos: - repo: https://github.com/astral-sh/ruff-pre-commit rev: v0.4.8 # Use the latest tag from https://github.com/astral-sh/ruff-pre-commit/releases hooks: - id: ruff - id: ruff-format Here you can check Ruff\u0026rsquo;s default configuration.\nFinally, install the Git hook:\n1 pre-commit install Now, every time you run git commit, Ruff will check and/or format your staged files automatically. If it requeries a manual change, you won\u0026rsquo;t be able to commit until you fix the code.\nYou can ignore pre-commit using -nm instead of -m, for example:\n1 git commit -nm \u0026#34;This commit hasn\u0026#39;t run pre-commit :(\u0026#34; You may also apply the hook to all files (not just the staged ones) using:\n1 pre-commit run --all-files Note: This will install the default config. Adapt it according to your needs.\nWith the previous setup, you will have the most basic Python project you‚Äôll need. Of course, if you are working on research code you will probably need other tools to track your experiments, make them reproducible, etc. And if it\u0026rsquo;s production code, you will need to add documentation, CD/CI, etc. But that could be covered in more detail in another post\u0026hellip; if I feel like it üòâ\nUntil then, here\u0026rsquo;s a song recommendation ‚ù§Ô∏è\n","date":"2025-07-20T00:00:00Z","permalink":"http://localhost:1313/posts/create-python-repo/","title":"How to create a simple Python project"}]